{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDw9dbACzTNh"
   },
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run perintah dibawah ini jika install selenium saja masih belum bekerja\n",
    "# !pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nQJwdBF1yyb"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Path ke GeckoDriver, sesuaikan dengan path di komputer\n",
    "gecko_path = \"D:\\\\geckodriver.exe\"  # Sesuaikan path ke GeckoDriver\n",
    "service = Service(executable_path=gecko_path)\n",
    "\n",
    "# Path ke Firefox executable\n",
    "firefox_path = \"C:\\\\Program Files\\\\Mozilla Firefox\\\\firefox.exe\"  # Lokasi Firefox.exe\n",
    "\n",
    "# Konfigurasi Firefox\n",
    "options = Options()\n",
    "options.binary_location = firefox_path  # Menetapkan lokasi binary Firefox\n",
    "options.headless = False  # Set ke True jika Anda tidak ingin membuka jendela browser\n",
    "\n",
    "def create_driver():\n",
    "    return webdriver.Firefox(service=service, options=options)\n",
    "\n",
    "def get_tweet(element):\n",
    "    try:\n",
    "        user = element.find_element(By.XPATH, \".//*[contains(text(), '@')]\").text\n",
    "        text = element.find_element(By.XPATH, \".//div[@lang]\").text\n",
    "        date = element.find_element(By.XPATH, \".//time\").get_attribute(\"datetime\")\n",
    "\n",
    "        try:\n",
    "            reply_to_user = element.find_element(By.XPATH, \".//div[@dir='ltr']//a[contains(@href, '/')]\").text\n",
    "        except:\n",
    "            reply_to_user = None\n",
    "\n",
    "        try:\n",
    "            tweet_link = element.find_element(By.XPATH, \".//a[@href and contains(@href, '/status/')]\").get_attribute(\"href\")\n",
    "        except:\n",
    "            tweet_link = None\n",
    "\n",
    "        try:\n",
    "            media_links = [media.get_attribute(\"src\") for media in element.find_elements(By.XPATH, \".//img[contains(@src, 'media')]\")]\n",
    "        except:\n",
    "            media_links = []\n",
    "\n",
    "        tweet_data = [user, text, date, tweet_link, media_links, reply_to_user]\n",
    "        return tweet_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def click_retry_button(driver):\n",
    "    try:\n",
    "        retry_button = driver.find_element(By.XPATH, \"//div[text()='Retry']/parent::div\")\n",
    "        retry_button.click()\n",
    "        print(\"Clicked the retry button\")\n",
    "        WebDriverWait(driver, 15).until(EC.invisibility_of_element_located((By.XPATH, \"//div[text()='Retry']\")))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def scrape_tweets(driver):\n",
    "    user_data = []\n",
    "    text_data = []\n",
    "    date_data = []\n",
    "    tweet_link_data = []\n",
    "    media_link_data = []\n",
    "    reply_to_data = []\n",
    "    tweet_ids = set()\n",
    "\n",
    "    scrolling = True\n",
    "    while scrolling and len(user_data) < 10000:\n",
    "        try:\n",
    "            print(\"Searching for tweets...\")\n",
    "            tweets = WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.XPATH, \"//article\")))\n",
    "            print(f\"Number of tweets found: {len(tweets)}\")\n",
    "            for tweet in tweets:\n",
    "                tweet_list = get_tweet(tweet)\n",
    "                if tweet_list:\n",
    "                    tweet_id = ''.join(tweet_list[:2])\n",
    "                    if tweet_id not in tweet_ids:\n",
    "                        tweet_ids.add(tweet_id)\n",
    "                        user_data.append(tweet_list[0])\n",
    "                        text_data.append(\" \".join(tweet_list[1].split()))\n",
    "                        date_data.append(tweet_list[2])\n",
    "                        tweet_link_data.append(tweet_list[3])\n",
    "                        media_link_data.append(tweet_list[4])\n",
    "                        reply_to_data.append(tweet_list[5])\n",
    "\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            WebDriverWait(driver, 30).until(lambda driver: driver.execute_script(\"return document.body.scrollHeight\") > last_height)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            click_retry_button(driver)\n",
    "\n",
    "            if new_height == last_height:\n",
    "                scrolling = False\n",
    "            else:\n",
    "                last_height = new_height\n",
    "                time.sleep(5)  # Tambahkan penundaan yang lebih lama\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scrolling or tweet retrieval: {e}\")\n",
    "            break\n",
    "\n",
    "    return user_data, text_data, date_data, tweet_link_data, media_link_data, reply_to_data\n",
    "\n",
    "driver = create_driver()\n",
    "web = 'https://x.com/search?q=foodfess2%20tomoro%202024%20lang%3Aid%20until%3A2024-07-01%20since%3A2023-07-01&src=typed_query&f=live' # Masukan link yang akan di cari\n",
    "driver.get(web)\n",
    "time.sleep(15)\n",
    "driver.save_screenshot('screenshot_before_search.png')\n",
    "\n",
    "user_data, text_data, date_data, tweet_link_data, media_link_data, reply_to_data = scrape_tweets(driver)\n",
    "\n",
    "driver.save_screenshot('screenshot_after_search.png')\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'User': user_data,\n",
    "    'Text': text_data,\n",
    "    'Date': date_data,\n",
    "    'Tweet Link': tweet_link_data,\n",
    "    'Media Links': media_link_data,\n",
    "    'Reply To': reply_to_data\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qk4Y-0OO2CuI"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_mentions(text):\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return mentions\n",
    "\n",
    "df['Target'] = df['User']\n",
    "df['Mentions'] = df['Text'].apply(extract_mentions)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3HYHqjV2MyP"
   },
   "outputs": [],
   "source": [
    "# Simpan kedalam csv\n",
    "df.to_csv('hasil-crawling.csv', index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
